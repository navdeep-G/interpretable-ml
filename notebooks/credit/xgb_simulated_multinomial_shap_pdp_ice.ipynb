{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering Transparency into Your Multinomial Machine Learning Model with Python (Based on simulated data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial partial dependence, ICE, and Shapley explanations\n",
    "In this notebook a gradient boosting machine (GBM) is trained to predict a multinomial target using a simulated dataset, Python, NumPy, Pandas, and XGBoost. First, the dataset is simulated. Then the multinomial model is trained. After the model is trained, partial dependence and individual conditional expectation (ICE) plots are used to analyze and verify the model's behavior. Finally an example of creating regulator mandated reason codes from high fidelity Shapley explanations for any model prediction is presented. This combination of partial dependence, ICE, and Shapley explanations is probably the most direct way to create an interpretable machine learning model today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                   \n",
    "import pandas as pd                 \n",
    "import xgboost as xgb                \n",
    "import shap\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMakerAndGetter(object):\n",
    "\n",
    "    \"\"\" This class makes randomly generated data sets. Generated data sets\n",
    "    currently contain 15 columns of inputs - 3 of which are categorical - and\n",
    "    one target variable. Randomly generated inputs are not correlated with\n",
    "    one another or the target. Users may select to the number of rows in the\n",
    "    generated data set, the type of target, and whether to save the generated\n",
    "    data.\n",
    "\n",
    "    :ivar seed: Random seed for enhanced reproducibility, default 12345.\n",
    "    :ivar nrows: Number of rows of the  generated data set, default 100000.\n",
    "    :ivar target: Can be 'numeric', 'binary', or multinomial, default 'binary'.\n",
    "    :ivar save: Save or not save the file, default not save.\n",
    "    :ivar noise: Adds noise to signal generating function, default False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed=None, nrows=None, target=None, save=None,\n",
    "                 one_function=None, noise=None):\n",
    "\n",
    "        # init properties\n",
    "        self.__ncols = 21\n",
    "        self.__ncats = 0 #Leave out categoricals for XGBoosts sake...\n",
    "\n",
    "        # set ivar defaults\n",
    "        if seed is None:\n",
    "            self.seed = 12345\n",
    "        else:\n",
    "            self.seed = seed\n",
    "\n",
    "        if nrows is None:\n",
    "            self.nrows = 100000\n",
    "        else:\n",
    "            self.nrows = nrows\n",
    "\n",
    "        if target is None:\n",
    "            self.target = 'binary'\n",
    "        else:\n",
    "            self.target = target\n",
    "\n",
    "        if save is None:\n",
    "            self.save = False\n",
    "        else:\n",
    "            self.save = save\n",
    "\n",
    "        if one_function is None:\n",
    "            self.one_function = False\n",
    "        else:\n",
    "            self.one_function = one_function\n",
    "\n",
    "        if noise is None:\n",
    "            self.noise = False\n",
    "        else:\n",
    "            self.noise = noise\n",
    "\n",
    "    @property\n",
    "    def ncols(self):\n",
    "\n",
    "        \"\"\" Number of variables of the generated data set, fixed at 15. \"\"\"\n",
    "\n",
    "        return self.__ncols\n",
    "\n",
    "    @property\n",
    "    def ncats(self):\n",
    "\n",
    "        \"\"\" Number of categorical variables in the generated data set,\n",
    "        fixed at 3. \"\"\"\n",
    "\n",
    "        return self.__ncats\n",
    "\n",
    "    def make_random(self):\n",
    "\n",
    "        \"\"\" This function makes a random data set with user defined number of\n",
    "        rows and type of target variable and can save the generated set as a\n",
    "        CSV file. The default setting is 100000*15 variables, in which 3\n",
    "        variables are categorical, plus one target column.\n",
    "\n",
    "        :return: Returns a Pandas Dataframe with user defined number of rows\n",
    "        and target variable type.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # set random seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # create numeric frame\n",
    "        n_num_cols = self.ncols - self.ncats\n",
    "        num_col_names = ['num' + str(i + 1) for i in range(0, n_num_cols)]\n",
    "        num_cols = pd.DataFrame(np.random.randn(self.nrows, n_num_cols),\n",
    "                                columns=num_col_names)\n",
    "\n",
    "        # make categorical frame\n",
    "        cat_col_names = map(lambda j: 'cat' + str(j + 1),\n",
    "                            range(0, self.ncats))\n",
    "        text_draw = [(letter * 8) for letter in string.ascii_uppercase[:7]]\n",
    "        cat_cols = pd.DataFrame(np.random.choice(text_draw, (self.nrows,\n",
    "                                                             self.ncats)),\n",
    "                                columns=cat_col_names)\n",
    "\n",
    "        # make target frame\n",
    "        if self.target == 'binary':\n",
    "            target_ = pd.DataFrame(np.random.choice([0, 1], size=self.nrows,\n",
    "                                                    p=[0.5, 0.5]),\n",
    "                                   columns=['target'])\n",
    "        elif self.target == 'multinomial':\n",
    "            target_ = pd.DataFrame(np.random.choice([0, 1, 2], size=self.nrows,\n",
    "                                                    p=[.333333333334, .333333333334, .333333333334]),\n",
    "                                   columns=['target'])\n",
    "        else:\n",
    "            target_ = pd.DataFrame(np.random.randint(100, size=self.nrows),\n",
    "                                   columns=['target'])\n",
    "\n",
    "        # column bind all frames together\n",
    "        frame = pd.concat([num_cols, cat_cols, target_], axis=1)\n",
    "\n",
    "        # add row_id\n",
    "        frame['row_id'] = frame.index\n",
    "\n",
    "        # conditionally save\n",
    "        if self.save:\n",
    "            frame.to_csv('random.csv', index=False)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def make_random_with_signal(self):\n",
    "\n",
    "        \"\"\" This function transforms the dataset generated by make_random. The\n",
    "        number of total variables are fixed at 15, with 3 of them being\n",
    "        categorical. 9 of the numeric variable, num1 to num9, were used to transform\n",
    "        the target. 3 of the numeric columns are totally random.\n",
    "        The generated set can also be saved.\n",
    "\n",
    "        :return: Returns a Pandas Dataframe with signal of user defined\n",
    "        number of rows and type of target.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # local constants and settings\n",
    "        cached_save = self.save\n",
    "        self.save = False\n",
    "\n",
    "        # make totally random frame\n",
    "        frame = self.make_random()\n",
    "\n",
    "        # function 1, on all rows\n",
    "        frame.loc[::1, 'target'] = np.abs(frame.loc[::1, 'num8']) *\\\n",
    "            np.square(frame.loc[::1, 'num9']) + frame.loc[::1, 'num4'] *\\\n",
    "            frame.loc[::1, 'num1']\n",
    "\n",
    "        frame.loc[::1, 'function'] = 1\n",
    "\n",
    "        # function 2, on mod 2 rows\n",
    "        if not self.one_function:\n",
    "            frame.loc[::2, 'target'] = np.sin(frame.loc[::2, 'num6']) -\\\n",
    "                np.sqrt(np.abs(frame.loc[::2, 'num7'])) - frame.loc[::2, 'num2'] *\\\n",
    "                frame.loc[::2, 'num3']\n",
    "\n",
    "            frame.loc[::2, 'function'] = 2\n",
    "        \n",
    "        if self.target != 'multinomial':\n",
    "            cut = frame.loc[:, 'target'].mean()\n",
    "\n",
    "        if self.noise:\n",
    "            frame.loc[::5, 'target'] = 1\n",
    "            frame.loc[::7, 'target'] = 0\n",
    "\n",
    "        # set the cut off point for binary target\n",
    "        if self.target == 'binary':\n",
    "            frame.loc[frame['target'] >= cut, 'target'] = 1\n",
    "            frame.loc[frame['target'] < cut, 'target'] = 0\n",
    "        \n",
    "        if self.target == 'multinomial':\n",
    "            frame.loc[frame['target']  <= 0, 'target'] = 0\n",
    "            frame.loc[(frame['target'] <= 1) & (frame['target'] > 0), 'target'] = 1\n",
    "            #frame.loc[(frame['target'] <= 2) & (frame['target'] > 1), 'target'] = 2\n",
    "            frame.loc[frame['target']  > 1, 'target'] = 2\n",
    "            \n",
    "        # conditionally save\n",
    "        if cached_save:\n",
    "            frame.to_csv('random_with_signal.csv', index=False)\n",
    "            self.save = cached_save\n",
    "\n",
    "        return frame\n",
    "\n",
    "    @classmethod\n",
    "    def get_percentile_dict(cls, y, id_, frame):\n",
    "\n",
    "        \"\"\" Returns the percentiles of a column, yhat, as the indices based on\n",
    "        another column id_.\n",
    "\n",
    "        :param y: Column in which to find percentiles.\n",
    "        :param id_: Id column that stores indices for percentiles of yhat.\n",
    "        :param frame: Pandas Dataframe containing y and id_.\n",
    "\n",
    "        :returns: Dictionary of percentile values and index column values.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # create a copy of frame and sort it by yhat\n",
    "        sort_df = frame.as_data_frame()\n",
    "        sort_df.sort_values(y, inplace=True)\n",
    "        sort_df.reset_index(inplace=True)\n",
    "\n",
    "        # find top and bottom percentiles\n",
    "        percentiles_dict = {0: sort_df.loc[0, id_],\n",
    "                            99: sort_df.loc[sort_df.shape[0] - 1, id_]}\n",
    "\n",
    "        # find 10th-90th percentiles\n",
    "        inc = sort_df.shape[0] // 10\n",
    "        for i in range(1, 10):\n",
    "            percentiles_dict[i * 10] = sort_df.loc[i * inc, id_]\n",
    "\n",
    "        return percentiles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known Signal Generating Function for Simulated Data\n",
    "The outcome, `y (target)`, is generated based on the following formula:\n",
    "\n",
    "$$y = x_1 * x_4 + |x_8| * (x_9)^2$$\n",
    "\n",
    "This indicates that `y (target)` is highly correlated to `x1`, `x4`, `x8`, and `x9` which translate to `num1`, `num4`, `num8`, and `num9` in the generated dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>num11</th>\n",
       "      <th>num12</th>\n",
       "      <th>num13</th>\n",
       "      <th>num14</th>\n",
       "      <th>num15</th>\n",
       "      <th>num16</th>\n",
       "      <th>num17</th>\n",
       "      <th>num18</th>\n",
       "      <th>num19</th>\n",
       "      <th>num20</th>\n",
       "      <th>num21</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "      <th>function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204708</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>-0.519439</td>\n",
       "      <td>-0.555730</td>\n",
       "      <td>1.965781</td>\n",
       "      <td>1.393406</td>\n",
       "      <td>0.092908</td>\n",
       "      <td>0.281746</td>\n",
       "      <td>0.769023</td>\n",
       "      <td>1.246435</td>\n",
       "      <td>1.007189</td>\n",
       "      <td>-1.296221</td>\n",
       "      <td>0.274992</td>\n",
       "      <td>0.228913</td>\n",
       "      <td>1.352917</td>\n",
       "      <td>0.886429</td>\n",
       "      <td>-2.001637</td>\n",
       "      <td>-0.371843</td>\n",
       "      <td>1.669025</td>\n",
       "      <td>-0.438570</td>\n",
       "      <td>-0.539741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.476985</td>\n",
       "      <td>3.248944</td>\n",
       "      <td>-1.021228</td>\n",
       "      <td>-0.577087</td>\n",
       "      <td>0.124121</td>\n",
       "      <td>0.302614</td>\n",
       "      <td>0.523772</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>1.343810</td>\n",
       "      <td>-0.713544</td>\n",
       "      <td>-0.831154</td>\n",
       "      <td>-2.370232</td>\n",
       "      <td>-1.860761</td>\n",
       "      <td>-0.860757</td>\n",
       "      <td>0.560145</td>\n",
       "      <td>-1.265934</td>\n",
       "      <td>0.119827</td>\n",
       "      <td>-1.063512</td>\n",
       "      <td>0.332883</td>\n",
       "      <td>-2.359419</td>\n",
       "      <td>-0.199543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.541996</td>\n",
       "      <td>-0.970736</td>\n",
       "      <td>-1.307030</td>\n",
       "      <td>0.286350</td>\n",
       "      <td>0.377984</td>\n",
       "      <td>-0.753887</td>\n",
       "      <td>0.331286</td>\n",
       "      <td>1.349742</td>\n",
       "      <td>0.069877</td>\n",
       "      <td>0.246674</td>\n",
       "      <td>-0.011862</td>\n",
       "      <td>1.004812</td>\n",
       "      <td>1.327195</td>\n",
       "      <td>-0.919262</td>\n",
       "      <td>-1.549106</td>\n",
       "      <td>0.022185</td>\n",
       "      <td>0.758363</td>\n",
       "      <td>-0.660524</td>\n",
       "      <td>0.862580</td>\n",
       "      <td>-0.010032</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.670216</td>\n",
       "      <td>0.852965</td>\n",
       "      <td>-0.955869</td>\n",
       "      <td>-0.023493</td>\n",
       "      <td>-2.304234</td>\n",
       "      <td>-0.652469</td>\n",
       "      <td>-1.218302</td>\n",
       "      <td>-1.332610</td>\n",
       "      <td>1.074623</td>\n",
       "      <td>0.723642</td>\n",
       "      <td>0.690002</td>\n",
       "      <td>1.001543</td>\n",
       "      <td>-0.503087</td>\n",
       "      <td>-0.622274</td>\n",
       "      <td>-0.921169</td>\n",
       "      <td>-0.726213</td>\n",
       "      <td>0.222896</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>-1.157719</td>\n",
       "      <td>0.816707</td>\n",
       "      <td>0.433610</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.010737</td>\n",
       "      <td>1.824875</td>\n",
       "      <td>-0.997518</td>\n",
       "      <td>0.850591</td>\n",
       "      <td>-0.131578</td>\n",
       "      <td>0.912414</td>\n",
       "      <td>0.188211</td>\n",
       "      <td>2.169461</td>\n",
       "      <td>-0.114928</td>\n",
       "      <td>2.003697</td>\n",
       "      <td>0.029610</td>\n",
       "      <td>0.795253</td>\n",
       "      <td>0.118110</td>\n",
       "      <td>-0.748532</td>\n",
       "      <td>0.584970</td>\n",
       "      <td>0.152677</td>\n",
       "      <td>-1.565657</td>\n",
       "      <td>-0.562540</td>\n",
       "      <td>-0.032664</td>\n",
       "      <td>-0.929006</td>\n",
       "      <td>-0.482573</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num1      num2      num3      num4      num5      num6      num7  \\\n",
       "0 -0.204708  0.478943 -0.519439 -0.555730  1.965781  1.393406  0.092908   \n",
       "1  0.476985  3.248944 -1.021228 -0.577087  0.124121  0.302614  0.523772   \n",
       "2 -1.541996 -0.970736 -1.307030  0.286350  0.377984 -0.753887  0.331286   \n",
       "3  0.670216  0.852965 -0.955869 -0.023493 -2.304234 -0.652469 -1.218302   \n",
       "4  1.010737  1.824875 -0.997518  0.850591 -0.131578  0.912414  0.188211   \n",
       "\n",
       "       num8      num9     num10     num11     num12     num13     num14  \\\n",
       "0  0.281746  0.769023  1.246435  1.007189 -1.296221  0.274992  0.228913   \n",
       "1  0.000940  1.343810 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757   \n",
       "2  1.349742  0.069877  0.246674 -0.011862  1.004812  1.327195 -0.919262   \n",
       "3 -1.332610  1.074623  0.723642  0.690002  1.001543 -0.503087 -0.622274   \n",
       "4  2.169461 -0.114928  2.003697  0.029610  0.795253  0.118110 -0.748532   \n",
       "\n",
       "      num15     num16     num17     num18     num19     num20     num21  \\\n",
       "0  1.352917  0.886429 -2.001637 -0.371843  1.669025 -0.438570 -0.539741   \n",
       "1  0.560145 -1.265934  0.119827 -1.063512  0.332883 -2.359419 -0.199543   \n",
       "2 -1.549106  0.022185  0.758363 -0.660524  0.862580 -0.010032  0.050009   \n",
       "3 -0.921169 -0.726213  0.222896  0.051316 -1.157719  0.816707  0.433610   \n",
       "4  0.584970  0.152677 -1.565657 -0.562540 -0.032664 -0.929006 -0.482573   \n",
       "\n",
       "   target  row_id  function  \n",
       "0     0.0       0       1.0  \n",
       "1     0.0       1       1.0  \n",
       "2     0.0       2       1.0  \n",
       "3     2.0       3       1.0  \n",
       "4     1.0       4       1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_ = DataMakerAndGetter(nrows=30000, target='multinomial', one_function=True, noise=True, seed=12345)\n",
    "data = ds_.make_random_with_signal()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign modeling roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = target\n",
      "X = ['num1', 'num2', 'num3', 'num4', 'num5', 'num6', 'num7', 'num8', 'num9', 'num10', 'num11', 'num12', 'num13', 'num14', 'num15', 'num16', 'num17', 'num18', 'num19', 'num20', 'num21']\n"
     ]
    }
   ],
   "source": [
    "#Assign target and inputs for GBM\n",
    "y = 'target'\n",
    "X = [name for name in data.columns if name not in [y,'row_id','function']]\n",
    "print('y =', y)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>num11</th>\n",
       "      <th>num12</th>\n",
       "      <th>num13</th>\n",
       "      <th>num14</th>\n",
       "      <th>num15</th>\n",
       "      <th>num16</th>\n",
       "      <th>num17</th>\n",
       "      <th>num18</th>\n",
       "      <th>num19</th>\n",
       "      <th>num20</th>\n",
       "      <th>num21</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "      <th>function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.012590</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>-0.000579</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>-0.002555</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>-0.003344</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.012990</td>\n",
       "      <td>-0.009158</td>\n",
       "      <td>-0.004660</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>-0.002435</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>-0.005233</td>\n",
       "      <td>0.877233</td>\n",
       "      <td>14999.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.998170</td>\n",
       "      <td>0.999171</td>\n",
       "      <td>0.996087</td>\n",
       "      <td>0.998874</td>\n",
       "      <td>0.996718</td>\n",
       "      <td>1.002234</td>\n",
       "      <td>0.994695</td>\n",
       "      <td>0.988403</td>\n",
       "      <td>0.993752</td>\n",
       "      <td>0.999242</td>\n",
       "      <td>0.997853</td>\n",
       "      <td>1.003659</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>1.005237</td>\n",
       "      <td>1.002263</td>\n",
       "      <td>1.007168</td>\n",
       "      <td>1.002283</td>\n",
       "      <td>0.993666</td>\n",
       "      <td>0.995449</td>\n",
       "      <td>0.996649</td>\n",
       "      <td>0.728638</td>\n",
       "      <td>8660.398374</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.263327</td>\n",
       "      <td>-3.892513</td>\n",
       "      <td>-4.150859</td>\n",
       "      <td>-3.793212</td>\n",
       "      <td>-3.916409</td>\n",
       "      <td>-3.628800</td>\n",
       "      <td>-3.776349</td>\n",
       "      <td>-4.501229</td>\n",
       "      <td>-3.558984</td>\n",
       "      <td>-4.092063</td>\n",
       "      <td>-4.106707</td>\n",
       "      <td>-4.179609</td>\n",
       "      <td>-4.071903</td>\n",
       "      <td>-4.301598</td>\n",
       "      <td>-3.905666</td>\n",
       "      <td>-3.926523</td>\n",
       "      <td>-3.924861</td>\n",
       "      <td>-3.957649</td>\n",
       "      <td>-5.057590</td>\n",
       "      <td>-4.148729</td>\n",
       "      <td>-4.250292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.654347</td>\n",
       "      <td>-0.670167</td>\n",
       "      <td>-0.674723</td>\n",
       "      <td>-0.676910</td>\n",
       "      <td>-0.666627</td>\n",
       "      <td>-0.679165</td>\n",
       "      <td>-0.670021</td>\n",
       "      <td>-0.665613</td>\n",
       "      <td>-0.676052</td>\n",
       "      <td>-0.667745</td>\n",
       "      <td>-0.678147</td>\n",
       "      <td>-0.671923</td>\n",
       "      <td>-0.659966</td>\n",
       "      <td>-0.690996</td>\n",
       "      <td>-0.679288</td>\n",
       "      <td>-0.680801</td>\n",
       "      <td>-0.670903</td>\n",
       "      <td>-0.673475</td>\n",
       "      <td>-0.666851</td>\n",
       "      <td>-0.665201</td>\n",
       "      <td>-0.672575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7499.750000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>-0.001632</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>-0.008698</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.009361</td>\n",
       "      <td>-0.011090</td>\n",
       "      <td>-0.015657</td>\n",
       "      <td>-0.006959</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>-0.003938</td>\n",
       "      <td>-0.006281</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.004740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14999.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.677308</td>\n",
       "      <td>0.674306</td>\n",
       "      <td>0.668459</td>\n",
       "      <td>0.680746</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.673963</td>\n",
       "      <td>0.671605</td>\n",
       "      <td>0.664063</td>\n",
       "      <td>0.676483</td>\n",
       "      <td>0.667730</td>\n",
       "      <td>0.672814</td>\n",
       "      <td>0.691745</td>\n",
       "      <td>0.677649</td>\n",
       "      <td>0.659190</td>\n",
       "      <td>0.670662</td>\n",
       "      <td>0.669312</td>\n",
       "      <td>0.689647</td>\n",
       "      <td>0.674770</td>\n",
       "      <td>0.659730</td>\n",
       "      <td>0.670049</td>\n",
       "      <td>0.662423</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22499.250000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.979780</td>\n",
       "      <td>4.100121</td>\n",
       "      <td>3.723884</td>\n",
       "      <td>3.867321</td>\n",
       "      <td>3.713553</td>\n",
       "      <td>4.569376</td>\n",
       "      <td>4.529198</td>\n",
       "      <td>3.990911</td>\n",
       "      <td>4.067701</td>\n",
       "      <td>4.390861</td>\n",
       "      <td>4.266457</td>\n",
       "      <td>4.481310</td>\n",
       "      <td>4.237124</td>\n",
       "      <td>3.927528</td>\n",
       "      <td>4.153813</td>\n",
       "      <td>4.034796</td>\n",
       "      <td>4.023149</td>\n",
       "      <td>3.636919</td>\n",
       "      <td>4.723747</td>\n",
       "      <td>3.958053</td>\n",
       "      <td>3.966642</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>29999.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               num1          num2          num3          num4          num5  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean       0.012590      0.002605     -0.000579      0.007462      0.005243   \n",
       "std        0.998170      0.999171      0.996087      0.998874      0.996718   \n",
       "min       -4.263327     -3.892513     -4.150859     -3.793212     -3.916409   \n",
       "25%       -0.654347     -0.670167     -0.674723     -0.676910     -0.666627   \n",
       "50%        0.006505      0.001560     -0.001632      0.008760      0.007613   \n",
       "75%        0.677308      0.674306      0.668459      0.680746      0.672513   \n",
       "max        4.979780      4.100121      3.723884      3.867321      3.713553   \n",
       "\n",
       "               num6          num7          num8          num9         num10  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      -0.002555      0.000550      0.002338      0.001087     -0.001469   \n",
       "std        1.002234      0.994695      0.988403      0.993752      0.999242   \n",
       "min       -3.628800     -3.776349     -4.501229     -3.558984     -4.092063   \n",
       "25%       -0.679165     -0.670021     -0.665613     -0.676052     -0.667745   \n",
       "50%       -0.008698      0.006025      0.004705      0.001685      0.001303   \n",
       "75%        0.673963      0.671605      0.664063      0.676483      0.667730   \n",
       "max        4.569376      4.529198      3.990911      4.067701      4.390861   \n",
       "\n",
       "              num11         num12         num13         num14         num15  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      -0.003344      0.005400      0.012990     -0.009158     -0.004660   \n",
       "std        0.997853      1.003659      1.002058      0.999407      1.005237   \n",
       "min       -4.106707     -4.179609     -4.071903     -4.301598     -3.905666   \n",
       "25%       -0.678147     -0.671923     -0.659966     -0.690996     -0.679288   \n",
       "50%       -0.004356      0.004913      0.009361     -0.011090     -0.015657   \n",
       "75%        0.672814      0.691745      0.677649      0.659190      0.670662   \n",
       "max        4.266457      4.481310      4.237124      3.927528      4.153813   \n",
       "\n",
       "              num16         num17         num18         num19         num20  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      -0.003395      0.009174      0.004225     -0.002435      0.002337   \n",
       "std        1.002263      1.007168      1.002283      0.993666      0.995449   \n",
       "min       -3.926523     -3.924861     -3.957649     -5.057590     -4.148729   \n",
       "25%       -0.680801     -0.670903     -0.673475     -0.666851     -0.665201   \n",
       "50%       -0.006959      0.003619     -0.003938     -0.006281     -0.000754   \n",
       "75%        0.669312      0.689647      0.674770      0.659730      0.670049   \n",
       "max        4.034796      4.023149      3.636919      4.723747      3.958053   \n",
       "\n",
       "              num21        target        row_id  function  \n",
       "count  30000.000000  30000.000000  30000.000000   30000.0  \n",
       "mean      -0.005233      0.877233  14999.500000       1.0  \n",
       "std        0.996649      0.728638   8660.398374       0.0  \n",
       "min       -4.250292      0.000000      0.000000       1.0  \n",
       "25%       -0.672575      0.000000   7499.750000       1.0  \n",
       "50%       -0.004740      1.000000  14999.500000       1.0  \n",
       "75%        0.662423      1.000000  22499.250000       1.0  \n",
       "max        3.966642      2.000000  29999.000000       1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe() #Display descriptive statistics for all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multinomial XGBoost model\n",
    "\n",
    "XGBoost is a very accurate, open source GBM library for regression and classification tasks. XGBoost can learn complex relationships between input variables and a target variable. XGBoost's early stopping functionality is also used to limit overfitting to the training data\n",
    "\n",
    "XGBoost is available from: https://github.com/dmlc/xgboost and the implementation of XGBoost is described in detail here: http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf.\n",
    "\n",
    "After training, global Shapley variable importance is calculated and displayed. To enhance trust in the GBM model, variable importance values should typically conform to human domain knowledge and reasonable expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulated dataset is split into training and test sets to monitor and prevent overtraining. Reproducibility is another important factor in creating trustworthy models, and randomly splitting datasets can introduce randomness in model predictions and other results. A random seed is used here to ensure the data split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data rows = 20946, columns = 24\n",
      "Test data rows = 9054, columns = 24\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12345) #Set random seed for reproducibility\n",
    "split_ratio = 0.7     #70%/30% train/test split\n",
    "\n",
    "#Execute split\n",
    "split = np.random.rand(len(data)) < split_ratio\n",
    "train = data[split]\n",
    "test = data[~split]\n",
    "\n",
    "#Summarize split\n",
    "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
    "print('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost GBM multinomial classifier\n",
    "To train an XGBoost multinomial classifier, the training and test data must be converted from Pandas DataFrames into SVMLight format. The `DMatrix()` function in the XGBoost package is used to convert the data. \n",
    "\n",
    "Because gradient boosting methods typically resample training data, an additional random seed is also specified for XGBoost using the `seed` paramter to create reproducible predictions, error rates, and variable importance values. To avoid overfitting, the `early_stopping_rounds` parameter is used to stop the training process after the test multiclass logloss fails to decrease for 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train[X], label=train[y])\n",
    "dtest = xgb.DMatrix(test[X], label=test[y])\n",
    "\n",
    "#Tuning parameters\n",
    "params = {\n",
    "    'objective': 'multi:softprob',              #Multinomial classification\n",
    "    'booster': 'gbtree',                        #Base learner will be decision tree\n",
    "    'eval_metric': 'mlogloss',                  #Stop training based on multinomial logloss\n",
    "    'eta': 0.01,                                #Learning rate\n",
    "    'subsample': 0.9,                           #Use 90% of rows in each decision tree\n",
    "    'colsample_bytree': 0.9,                    #Use 90% of columns in each decision tree\n",
    "    'max_depth': 6,                             #Allow decision trees to grow to depth of 6\n",
    "    'seed': 12345,                              #Set random seed for reproducibility\n",
    "    'num_class': 3,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "#Watchlist is used for early stopping\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "#Train model\n",
    "xgb_model = xgb.train(params,                   #Set tuning parameters from above                   \n",
    "                      dtrain,                   #Training data\n",
    "                      1000,                     #Maximum of 1000 iterations (trees)\n",
    "                      evals=watchlist,          #Use watchlist for early stopping \n",
    "                      early_stopping_rounds=50, #Stop after 50 iterations (trees) without decrease in mlogloss\n",
    "                      verbose_eval=False)       #Display iteration progress or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Shapley variable importance\n",
    "By setting pred_contribs=True, XGBoost's predict() function will return Shapley values for each row and for each class of the test set. Instead of relying on traditional single-value variable importance measures, local Shapley values for each input will be plotted below to get a more holistic and consisent measurement for the global importance of each input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtest is DMatrix\n",
    "#shap_values is Numpy array\n",
    "shap_values = xgb_model.predict(dtest, pred_contribs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Global Shapley variable importance summary\n",
    "\n",
    "#### A good way to look at multinomial Shapley variable importance involves getting the mean absolute Shapley value across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set all shapley values to their absolute value\n",
    "shap_values_abs = np.absolute(shap_values[:,:,:-1])\n",
    "\n",
    "#Plot mean absolute shapley across all classes for global shapley variable importance\n",
    "plt.title(\"Global Shapley Variable Importance (on simulated data)\")\n",
    "shap.summary_plot(shap_values_abs.mean(axis=1), test[xgb_model.feature_names], plot_type=\"bar\", show=False)\n",
    "plt.savefig('./images/global_shapley_simulated.png',bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#To show in notebook\n",
    "plt.title(\"Global Shapley Variable Importance (on simulated data)\")\n",
    "shap.summary_plot(shap_values_abs.mean(axis=1), test[xgb_model.feature_names], plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Shapley variable importance summary per `target` class outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot signed Shapley variable importance summary for a specific class (class 0, class 1, class2)\n",
    "for i in range(0,3):\n",
    "    plt.title(\"Shapley Plot for Class (on simulated data): \" + str(i))\n",
    "    shap.summary_plot(shap_values[:, i, :-1], test[xgb_model.feature_names], show=False)\n",
    "    plt.savefig('./images/shapley_class_simulated' + str(i) + \".png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #To show in notebook\n",
    "    plt.title(\"Shapley Plot for Class (on simulated data): \" + str(i))\n",
    "    shap.summary_plot(shap_values[:, i, :-1], test[xgb_model.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set\n",
    "preds = xgb_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display predictions as a Pandas frame\n",
    "#Each column represents a class outcome for target: 0, 1, 2\n",
    "pd.DataFrame(preds).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at model accuracy\n",
    "pred_prob = xgb_model.predict(dtest).reshape(test[y].shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != test[y]) / test[y].shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each line we need to select that column \n",
    "#where the probability is the highest\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get actual values of target in test set\n",
    "test_true = test[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(test_true, best_preds)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at precision and F1 with micro and macro averaging\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "print(\"Macro Precision: %s\" % str(precision_score(test_true, best_preds, average='macro')))\n",
    "print(\"Micro Precision: %s\" % str(precision_score(test_true, best_preds, average='micro')))\n",
    "\n",
    "print(\"Macro F1: %s\" % str(f1_score(test_true, best_preds, average='macro')))\n",
    "print(\"Micro F1: %s\" % str(f1_score(test_true, best_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating partial dependence for a multinomial model\n",
    "Partial dependence plots are used to view the global, average prediction behavior of a variable under the multinomial model. Multinomial partial dependence plots show the average prediction per class of the multinomial model as a function of specific values of an input variable of interest, indicating how the GBM predictions per class outcome change based on the values of the input variable of interest, while taking nonlinearity into consideration and averaging out the effects of all other input variables. Partial dependence plots enable increased transparency into the GBM's mechanisms and enable validation and debugging of the GBM by comparing a variable's average predictions per class across its domain to known standards and reasonable expectations. Partial dependence plots are described in greater detail in The Elements of Statistical Learning, section 10.13: https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for calculating multinomial partial dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_dep_multinomial(xs, frame, model, resolution=20, bins=None, num_classes=0):\n",
    "    \n",
    "    \"\"\" Creates Pandas DataFrame containing partial dependence for a \n",
    "        single variable.\n",
    "    \n",
    "    Args:\n",
    "        xs: Variable for which to calculate partial dependence.\n",
    "        frame: Pandas DataFrame for which to calculate partial dependence.\n",
    "        model: XGBoost model for which to calculate partial dependence.\n",
    "        resolution: The number of points across the domain of xs for which \n",
    "                    to calculate partial dependence, default 20.\n",
    "        bins: List of values at which to set xs, default 20 equally-spaced \n",
    "              points between column minimum and maximum.\n",
    "        num_classes: Number of classes in outcome\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame containing partial dependence values per class.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #Turn off pesky Pandas copy warning\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    #Initialize empty Pandas DataFrame with correct column names\n",
    "    class_name_columns = [0] * (num_classes + 1)\n",
    "    class_name_columns[0] = xs\n",
    "    for i in range(1, num_classes+1):\n",
    "        class_name_columns[i] = 'partial_dependence_' + str(i-1)\n",
    "    par_dep_frame = pd.DataFrame(columns=class_name_columns)\n",
    "    \n",
    "    #Cache original column values \n",
    "    col_cache = frame.loc[:, xs].copy(deep=True)\n",
    "  \n",
    "    #Determine values at which to calculate partial dependence\n",
    "    if bins == None:\n",
    "        min_ = frame[xs].min()\n",
    "        max_ = frame[xs].max()\n",
    "        by = (max_ - min_)/resolution\n",
    "        bins = np.arange(min_, max_, by)\n",
    "        \n",
    "    #Calculate partial dependence  \n",
    "    #by setting column of interest to constant \n",
    "    #and scoring the altered data and taking the mean of the predictions per class\n",
    "    row_idx = 0 #Keep track of row index for proper assignment \n",
    "    #Per bin\n",
    "    for j in bins:\n",
    "        frame.loc[:, xs] = j\n",
    "        dframe = xgb.DMatrix(frame)\n",
    "        par_dep_i = pd.DataFrame(model.predict(dframe))\n",
    "        #Per class\n",
    "        for i in range(0, num_classes):\n",
    "            par_dep_frame.loc[row_idx, xs] = j\n",
    "            par_dep_j = par_dep_i[i].mean()\n",
    "            par_dep_frame.loc[row_idx, 'partial_dependence_' + str(i)] = par_dep_j\n",
    "        row_idx+=1 #Increment and go onto next row\n",
    "        \n",
    "    #Return input frame to original cached state    \n",
    "    frame.loc[:, xs] = col_cache\n",
    "\n",
    "    return par_dep_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate partial dependence for the most important input variable(s) in the GBM\n",
    "`num9` is the most important variable according to global Shapley variable importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_num9 = par_dep_multinomial(xs=\"num9\", frame=test[X], model=xgb_model, resolution=20, bins=None, num_classes=3)\n",
    "pdp_num9.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting multinomial partial dependence\n",
    "Plotting the partial dependence per class is a good way of getting a global view of the multinomial model in question. \n",
    "\n",
    "**Note: If the number of outcome classes > 10, then we would only take the top 10 most common classes by default.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multinomial_pdp(pdp_data, xs):\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(0, 3):\n",
    "        pdp_data.plot(x=xs,y=\"partial_dependence_\"+str(i), ax=ax)\n",
    "    plt.title(\"Multinomial Partial Dependency Plot (simulated data)\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "    plt.savefig(\"./images/global_pdp_simulated.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP for `num9` across all class outcomes for `target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multinomial_pdp(pdp_num9, \"num9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting partial dependence for a specific class outcome of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdp_class_outcome(pdp_data, xs, class_category):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"Partial Dependency Plot for Outcome (on simulated data): \" + str(class_category))\n",
    "    pdp_data.plot(x=xs,y=\"partial_dependence_\"+str(class_category), ax=ax)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "    plt.savefig(\"./images/pdp_class_simulated\" + str(class_category) + \".png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP for `num9` and `target` outcome, `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdp_class_outcome(pdp_num9, \"num9\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP for `num9` and  all `target`  outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    plot_pdp_class_outcome(pdp_num9, \"num9\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Individual Conditional Expectation (ICE) for a multinomial model\n",
    "Individual conditional expectation (ICE) plots, a newer and less well-known adaptation of partial dependence plots, can be used to create more localized explanations for a single observation of data using the same basic ideas as partial dependence plots. ICE is also a type of nonlinear sensitivity analysis in which the model predictions for a single observation are measured while a feature of interest is varied over its domain. ICE increases understanding and transparency by displaying the nonlinear behavior of the GBM. ICE also enhances trust, accountability, and fairness by enabling comparisons of nonlinear behavior to human domain knowledge and reasonable expectations. ICE, as a type of sensitivity analysis, can also engender trust when model behavior on simulated or extreme data points is acceptable. A detailed description of ICE is available in this arXiv preprint: https://arxiv.org/abs/1309.6392."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for finding percentiles of predictions\n",
    "ICE can be calculated for any row in the training or test data, but without intimate knowledge of a data source it can be difficult to know where to apply ICE. Calculating and analyzing ICE curves for every row of training and test data set can be overwhelming, even for the example credit card default dataset. One place to start with ICE is to calculate ICE curves at every decile of predicted probabilities in a dataset, giving an indication of local prediction behavior across the dataset. The function below finds and returns the row indices for the maximum, minimum, and deciles of one column in terms of another -- in this case, the model predictions for being in class 0 and the row identifier (row_id), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile_dict(yhat, id_, frame):\n",
    "\n",
    "    \"\"\" Returns the percentiles of a column, yhat, as the indices based on \n",
    "        another column id_.\n",
    "    \n",
    "    Args:\n",
    "        yhat: Column in which to find percentiles.\n",
    "        id_: Id column that stores indices for percentiles of yhat.\n",
    "        frame: Pandas DataFrame containing yhat and id_. \n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of percentile values and index column values.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Create a copy of frame and sort it by yhat\n",
    "    sort_df = frame.copy(deep=True)\n",
    "    sort_df.sort_values(yhat, inplace=True)\n",
    "    sort_df.reset_index(inplace=True)\n",
    "    \n",
    "    #Find top and bottom percentiles\n",
    "    percentiles_dict = {}\n",
    "    percentiles_dict[0] = sort_df.loc[0, id_]\n",
    "    percentiles_dict[99] = sort_df.loc[sort_df.shape[0]-1, id_]\n",
    "\n",
    "    #Find 10th-90th percentiles\n",
    "    inc = sort_df.shape[0]//10\n",
    "    for i in range(1, 10):\n",
    "        percentiles_dict[i * 10] = sort_df.loc[i * inc,  id_]\n",
    "\n",
    "    return percentiles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find some percentiles of yhat for a particular `target` outcome in the test data\n",
    "The values for row_id that correspond to the maximum, minimum, and deciles of being in class 0 are displayed below. ICE will be calculated for the rows of the test dataset associated with these ID values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentiles_for_class(dtest, class_category):\n",
    "    #Merge GBM predictions onto test data\n",
    "    yhat_test = pd.concat([test.reset_index(drop=True), pd.DataFrame(xgb_model.predict(dtest)[:,class_category])], \n",
    "                          axis=1)\n",
    "    yhat_test = yhat_test.rename(columns={0:'p_class_' + str(class_category)})\n",
    "\n",
    "    #Find percentiles of predictions\n",
    "    percentile_dict = get_percentile_dict('p_class_' + str(class_category), 'row_id', yhat_test)\n",
    "\n",
    "    #Display percentiles dictionary\n",
    "    #ID values for rows\n",
    "    #from lowest prediction \n",
    "    #to highest prediction\n",
    "    return percentile_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_dict = get_percentiles_for_class(dtest, 0)\n",
    "percentile_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ICE curve values for `num9` against a particular `target` outcome in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ice_curves(pdp_data, xs, id_col, class_category, num_classes):\n",
    "    \"\"\" Creates Pandas DataFrame containing partial dependence and ICE\n",
    "    for a single variable\n",
    "    \n",
    "    Args:\n",
    "        pdp_data: Data contain partial dependence for a specific variable\n",
    "        xs: Variable for which to calculate partial dependence.\n",
    "        id_col: Column name containing percentiles\n",
    "        class_category: Class outcome of interest (an int ranging from 0-num_classes)\n",
    "        num_classes: Number of classes in outcome\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame containing partial dependence and ICE values for a specific class outcome.\n",
    "        \n",
    "    \"\"\"\n",
    "    #Retreive bins from original partial dependence calculation\n",
    "    bins = list(pdp_data[xs])\n",
    "    pdp_ice_data = pdp_data[[xs, \"partial_dependence_\" + str(class_category)]]\n",
    "    #Fr each percentile in percentile_dict\n",
    "    #create a new column in the par_dep frame \n",
    "    #representing the ICE curve for that percentile\n",
    "    #and the variables of interest\n",
    "    for i in sorted(percentile_dict.keys()):\n",
    "\n",
    "        col_name = 'Percentile_' + str(i)\n",
    "\n",
    "        # ICE curves for target across percentiles at bin intervals\n",
    "        pdp_ice_data[col_name] = par_dep_multinomial(xs, \n",
    "                                                  test[test[id_col] == int(percentile_dict[i])][X],  \n",
    "                                                  xgb_model, \n",
    "                                                  bins=bins,\n",
    "                                                  num_classes=num_classes)['partial_dependence_'+str(class_category)]\n",
    "    return pdp_ice_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ICE curve values for `num9` against `target` outcome, `0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_num9_class_zero = calc_ice_curves(pdp_num9, \"num9\", \"row_id\", 0, 3)\n",
    "pdp_num9_class_zero.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting partial dependence and ICE for particular `target` outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_par_dep_ICE(xs, par_dep_frame, class_index):\n",
    "\n",
    "    \n",
    "    \"\"\" Plots ICE overlayed onto partial dependence for a single variable.\n",
    "    \n",
    "    Args: \n",
    "        xs: Name of variable for which to plot ICE and partial dependence.\n",
    "        par_dep_frame: Name of Pandas DataFrame containing ICE and partial\n",
    "                       dependence values.\n",
    "        class_index: Class outcome of interest. XGBoost see's this as a value from 0-N where N is number of classes\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #Plot ICE curves\n",
    "    par_dep_frame.drop('partial_dependence_' + str(class_index), axis=1).plot(x=xs, \n",
    "                                                                 colormap='gnuplot',\n",
    "                                                                 ax=ax)\n",
    "\n",
    "    #Overlay partial dependence, annotate plot\n",
    "    par_dep_frame.plot(title='Partial Dependence and ICE for ' + str(xs) \n",
    "                              + ' with class ' + str(class_index) + ' as an outcome',\n",
    "                       x=xs, \n",
    "                       y='partial_dependence_' + str(class_index),\n",
    "                       style='r-', \n",
    "                       linewidth=3, \n",
    "                       ax=ax)\n",
    "\n",
    "    #Add legend\n",
    "    _ = plt.legend(bbox_to_anchor=(1.05, 0),\n",
    "                   loc=3, \n",
    "                   borderaxespad=0.)\n",
    "    \n",
    "    plt.savefig(\"./images/pdp_ice_class_simulated\" + str(class_index) + \".png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PDP & ICE curves for `num9` against being in `target` outcome, `0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_par_dep_ICE('num9', pdp_num9_class_zero, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PDP and ICE curves for `num9` against every class outcome in `target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "    plot_par_dep_ICE('num9', calc_ice_curves(pdp_num9, \"num9\", \"row_id\", i, 3), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate reason codes using the Shapley method\n",
    "Now that global behavior of the GBM has been verified and compared against domain knowledge and reasonable expectations, a method called Shapley explanations will be used to calculate the local variable importance for any one prediction: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions. Shapley explanations are the only possible consistent local variable importance values. (Here consistency means that if a variable is more important than another variable in a given prediction, the more important variable's Shapley value will not be smaller in magnitude than the less important variable's Shapley value.) Very crucially Shapley values also always sum to the actual prediction of the XGBoost model. When used in a model-specific context for decision tree models, Shapley values are likely the most accurate known local variable importance method available today. In this notebook, XGBoost itself is used to create Shapley values with the pred_contribs parameter to predict(), but the shap package is also available for other types of models: https://github.com/slundberg/shap.\n",
    "\n",
    "The numeric Shapley values in each column are an estimate of how much each variable contributed to each prediction. Shapley contributions can indicate how a variable and its values were weighted in any given decision by the model. These values are crucially important for machine learning interpretability and are related to \"local feature importance\", \"reason codes\", or \"turn-down codes.\" The latter phrases are borrowed from credit scoring. Credit lenders in the U.S. must provide reasons for automatically rejecting a credit application. Reason codes can be easily extracted from Shapley local variable contribution values by ranking the variables that played the largest role in any given decision.\n",
    "\n",
    "To find the index corresponding to a particular row of interest later, the index of the test DataFrame is reset to begin at 0 and increase sequentially. Without resetting the index, the test DataFrame row indices still correspond to the original raw data from which the test set was sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot Shapley values per class for a particular row at a certain decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shapley_row(decile, row, class_index=None):\n",
    "    if class_index is None:\n",
    "        for i in range(0,3):\n",
    "            s_df = pd.DataFrame(shap_values[row.index[0], i][:-1].reshape(21, 1), columns=['Reason Codes'], index=X)\n",
    "            s_df.sort_values(by='Reason Codes', inplace=True)\n",
    "\n",
    "            _ = s_df.plot(kind='bar', \n",
    "                              title='Reason Codes for Being in Class: ' + str(i) + ' (model predictions decile: ' + str(decile) + ')' + '\\n', \n",
    "                              legend=False)\n",
    "            plt.savefig(\"./images/shapley_simulated\" + str(decile) + \"_\" + str(i) + \".png\", bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    else:\n",
    "        s_df = pd.DataFrame(shap_values[row.index[0], class_index][:-1].reshape(21, 1), columns=['Reason Codes'], index=X)\n",
    "        s_df.sort_values(by='Reason Codes', inplace=True)\n",
    "\n",
    "        _ = s_df.plot(kind='bar', \n",
    "                  title='Reason Codes for Model Prediction: ' + str(class_index) + ' (model predictions decile: ' + str(decile) + ')' + '\\n', \n",
    "                  legend=False)\n",
    "        plt.savefig(\"./images/shapley_simulated\" + str(decile) + \"_\" + str(class_index) + \".png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10th decile of model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decile = 10\n",
    "row = test[test['row_id'] == percentile_dict[decile]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Local Shapley variable importance summary\n",
    "\n",
    "#### A good way to look at multinomial Shapley variable importance involves getting the mean absolute Shapley value across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set all shapley values to their absolute value\n",
    "shap_values_abs_row = np.absolute(shap_values[row.index[0],:,:-1])\n",
    "\n",
    "#Plot mean absolute shapley across all classes for local shapley variable importance\n",
    "plt.title(\"Local Shapley Variable Importance for 10th decile (on simulated data)\")\n",
    "shap.summary_plot(shap_values_abs_row, test[xgb_model.feature_names], plot_type=\"bar\", show=False)\n",
    "plt.savefig('./images/local_shapley_10decile_simulated.png',bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#To show in notebook\n",
    "#To show in notebook\n",
    "plt.title(\"Local Shapley Variable Importance for 10th decile of model predictions (on simulated data)\")\n",
    "shap.summary_plot(shap_values_abs_row, test[xgb_model.feature_names], plot_type=\"bar\", show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Shapley variable importance per `target` outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_row(decile, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get model predictions for minority class and majority class based on predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(row, X):\n",
    "    #Double check what the model actually predicted for this row\n",
    "    row_pred = xgb_model.predict(xgb.DMatrix(row[X]))\n",
    "    print(\"Predicted class probabilies: %s\" % str(row_pred[0]))\n",
    "    print(\"Smallest class probability based on model prediction: %s\" % str(np.asarray([np.argmin(line) for line in row_pred])[0]))\n",
    "    print(\"Highest class probability based on model prediction: %s\" % str(np.asarray([np.argmax(line) for line in row_pred])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What class does the model predict this row to be in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred(row, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley reason codes for model prediction of this row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_row(decile, row, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row #Helps understand reason codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90th decile of model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decile = 90\n",
    "row = test[test['row_id'] == percentile_dict[decile]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Local Shapley variable importance summary \n",
    "\n",
    "#### A good way to look at multinomial Shapley variable importance involves getting the mean absolute Shapley value across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set all shapley values to their absolute value\n",
    "shap_values_abs_row = np.absolute(shap_values[row.index[0],:,:-1])\n",
    "\n",
    "#Plot mean absolute shapley across all classes for local shapley variable importance\n",
    "plt.title(\"Local Shapley Variable Importance for 90th decile (on simulated data)\")\n",
    "shap.summary_plot(shap_values_abs_row, test[xgb_model.feature_names], plot_type=\"bar\", show=False)\n",
    "plt.savefig('./images/local_shapley_90decile_simulated.png',bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#To show in notebook\n",
    "plt.title(\"Local Shapley Variable Importance for 90th decile of model predictions (on simulated data)\")\n",
    "shap.summary_plot(shap_values_abs_row, test[xgb_model.feature_names], plot_type=\"bar\", show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Shapley variable importance per `target` outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_row(decile, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What class does the model predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred(row, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley Reason codes for model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_row(decile, row, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row #Helps understand reason codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out predictions, predicted class, and original columns to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(xgb_model.predict(dtest))\n",
    "preds.columns = ['prob_target_class' + str(col) for col in preds.columns]\n",
    "test_with_preds = pd.concat([test.reset_index(drop=True), preds], axis=1)\n",
    "test_with_preds = test_with_preds.drop(['row_id', 'function'], axis=1)\n",
    "test_with_preds[\"pred_target\"] = test_with_preds[preds.columns].idxmax(axis=1).str.split('_').str.get(2)\n",
    "test_with_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write train, test, and test with preds to csv\n",
    "test_with_preds.to_csv(\"./output/simulated_mult_preds.csv\", index=False)\n",
    "train.to_csv(\"./data/simulated_train.csv\", index=False)\n",
    "test.to_csv(\"./data/simulated_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "env_mli",
   "language": "python",
   "name": "env_mli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
