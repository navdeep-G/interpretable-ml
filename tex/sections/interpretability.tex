\section{Interpretability}

\begin{frame}[c]
\Huge{\centerline{Interpretability}}
\end{frame}
%----------------------------------------------------------------------------------------

\begin{frame}\frametitle{Model Complexity}

	\begin{itemize}
		\item The more complex a function, the more difficult it is to explain. Simple functions can be used to explain more complex functions, however not all explanatory techniques are a good match for all types of models. 
		\bigskip
		\item When interpreting a complex model, it is important to verify that your interpretation techniques:
		\begin{itemize}
			\item are appropriate for the response function complexity of the model.
			\item are appropriate for global and local interpretation.
			\item make sense given real-world context (aka match domain expertise).
			\item provide explanations that match the explanations of other interpretation techniques.
		\end{itemize}
		\end{itemize}
\end{frame}

%----------------------------------------------------------------------------------------

\begin{frame}\frametitle{Model Complexity (Cont.)}	
	\begin{itemize}
		\item \textbf{7} different techniques are presented to help interpret the results of your complex model. These techniques should be reviewed individually and within the general context of all other techniques. Depending on the relationship between your input training data and your target, certain interpretation techniques will be more reliable than others. These 7 techniques fall into two categories \textbf{model-agnostic} and \textbf{model-specific}, explained in the next section.
		\bigskip
		\item The \textbf{7} techniques were chosen based on the essential questions they could answer about a complex model, including but not limited to:
		\begin{itemize}
			\item how features are handled
			\item how interactions are handled
			\item the impact of individual features
			\item how to create reason codes
		\end{itemize}
	\end{itemize}
\end{frame}


%----------------------------------------------------------------------------------------

\begin{frame}\frametitle{Specific Interpretability Techniques}

	\begin{itemize}
		\item \textbf{Model Agnostic:} Techniques to interpret the inner workings of any model. Requires surrogate models (detailed in next section), which can degrade the interpretation's quality. Since there is no guarantee that a surrogate model reflects the decisions of a complex model, it is important to verify how well a surrogate model fits the results of the complex model before using it as a method of interpretation.
		\begin{itemize} 
			\item Decision Tree \textbf{Surrogate} Models
			\item Partial Dependence Plots
			\item Individual Conditional Expectation Plots
			\item LOCO
		\end{itemize} 
		\bigskip
		\item \textbf{Model Specific:} Techniques that are attributed to and require specific models.
		\begin{itemize}
			\item Random Forest or Gradient Boosting Variable Importance.
			\item Shapley: Tree Shap Implementation.
			\item Local Interpretable Model-agnostic Explanations (LIME).
		\end{itemize} 
	\end{itemize}
\end{frame}
